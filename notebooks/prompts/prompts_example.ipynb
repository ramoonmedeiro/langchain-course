{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts no LangChain\n",
    "\n",
    "Quando trata-se de IA generativa de texto, todas as LLMs necessitam de um input. \n",
    "\n",
    "O input é o que chamamos de \"prompt\", que é a frase ou texto que a LLM irá completar. A qualidade do input é crucial para a qualidade da resposta da LLM.\n",
    "\n",
    "O Langchain possui diversas interfaces para a criação desses templates, que é o que veremos neste notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM utilizada = Gemini-pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=gemini_api_key, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Template\n",
    "\n",
    "Com esta classe, você consegue criar templates bastante complexos para a geração de prompts de uma maneira simples.\n",
    "\n",
    "O que necessita é de um template (Uma instrução ou qualquer texto que você deseja que seja preenchido) e de um conjunto de palavras-chave que irão preencher o template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Você é o melhor programador da linguagem Python. Você irá auxiliar o usuário à realizar tarefas para esta linguagem \n",
      "em específico. Abaixo entre aspas está o comando:\n",
      "\n",
      "\n",
      "'''Crie uma função onde eu faço uma requisição numa URL com o requests'''\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Você é o melhor programador da linguagem {linguagem}. Você irá auxiliar o usuário à realizar tarefas para esta linguagem \n",
    "em específico. Abaixo entre aspas está o comando:\n",
    "\n",
    "\n",
    "'''{comando}'''\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"linguagem\", \"comando\"]\n",
    ")\n",
    "\n",
    "print(prompt.format(\n",
    "    linguagem=\"Python\",\n",
    "    comando=\"Crie uma função onde eu faço uma requisição numa URL com o requests\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_llm = llm.invoke(prompt.format(\n",
    "    linguagem=\"Python\",\n",
    "    comando=\"Crie uma função onde eu faço uma requisição numa URL com o requests\"\n",
    "))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "import requests\n",
      "\n",
      "def make_request(url):\n",
      "  \"\"\"Faz uma requisição GET para a URL fornecida e retorna a resposta.\n",
      "\n",
      "  Args:\n",
      "    url: A URL para a qual fazer a requisição.\n",
      "\n",
      "  Returns:\n",
      "    A resposta da requisição.\n",
      "  \"\"\"\n",
      "\n",
      "  response = requests.get(url)\n",
      "  return response\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(resp_llm.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Prompt Template\n",
    "\n",
    "Muito parecido com o Prompt Template, mas com a diferença de que ele é mais voltado para a criação de prompts para conversas em si. Você passa a mensagem e o tipo de papel que o chatbot ou o usuário irá desempenhar.\n",
    "\n",
    "Tipos de papéis (roles) disponíveis:\n",
    "- Human: Simulando resposta de um ser humano.\n",
    "- System: Mensagem que geralmente vem como instrução geral.\n",
    "- IA: Resposta de um chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='Você é um ótimo cozinheiro e quero que você se comporte e dê respostas apenas nos ingredientes (molho de tomate, cebola e massa) fornecido pelo usuário.'), HumanMessage(content='Dado molho de tomate, cebola e massa, crie 2 receitas detalhando como fazer.')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "messages = [\n",
    "    (\"system\", \"Você é um ótimo cozinheiro e quero que você se comporte e dê respostas apenas nos ingredientes ({ingredientes}) fornecido pelo usuário.\"),\n",
    "    (\"human\", \"Dado {ingredientes}, crie 2 receitas detalhando como fazer.\")\n",
    "]\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(messages=messages)\n",
    "print(chat_prompt.format_messages(ingredientes=\"molho de tomate, cebola e massa\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot Prompt Template\n",
    "\n",
    "Neste tipo de prompt, nós podemos passar exemplos de como a LLM deve completar o texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "exemplos = [\n",
    "    {\n",
    "        \"sentença\":\"O filme foi bom\",\n",
    "        \"resposta\": \"Reação: Positiva\"\n",
    "    },\n",
    "    {\n",
    "        \"sentença\":\"O filme não foi ruim e nem bom, mas poderia ser melhor\",\n",
    "        \"resposta\": \"Reação: Neutra\"\n",
    "    },\n",
    "    {\n",
    "        \"sentença\":\"Não acredito que desperdicei tempo e dinheiro para assistir esse filme\",\n",
    "        \"resposta\": \"Reação: Negativa\"\n",
    "    },\n",
    "]\n",
    "\n",
    "exemplo_prompt = PromptTemplate(\n",
    "    template=\"{sentença}\\n{resposta}\\n\",\n",
    "    input_variables=[\"sentença\", \"resposta\"]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=exemplos,\n",
    "    example_prompt=exemplo_prompt,\n",
    "    suffix=\"sentença: {sentença}\",\n",
    "    input_variables=[\"sentença\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O filme foi bom\n",
      "Reação: Positiva\n",
      "\n",
      "\n",
      "O filme não foi ruim e nem bom, mas poderia ser melhor\n",
      "Reação: Neutra\n",
      "\n",
      "\n",
      "Não acredito que desperdicei tempo e dinheiro para assistir esse filme\n",
      "Reação: Negativa\n",
      "\n",
      "\n",
      "sentença: O filme é longo, mas eu gostei. Valeu muito a pena\n"
     ]
    }
   ],
   "source": [
    "print(few_shot_prompt.format(sentença=\"O filme é longo, mas eu gostei. Valeu muito a pena\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reação: Positiva\n"
     ]
    }
   ],
   "source": [
    "resp_llm = llm.invoke(few_shot_prompt.format(sentença=\"O filme é longo, mas eu gostei. Valeu muito a pena\"))\n",
    "print(resp_llm.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-examples-NuBMPSch-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
