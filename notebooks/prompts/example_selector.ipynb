{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seletores de Exemplos (Examples Selectors)\n",
    "\n",
    "Imagine que você possua diversos exemplos para passar como contexto para o seu prompt. Neste caso, você não poderia\n",
    "passar todos os exemplos, pois isso poderia confundir o modelo. O ideal seria passar apenas os exemplos\n",
    "mais relevantes para o contexto que você deseja. Para isso, você pode utilizar os seletores de exemplos.\n",
    "\n",
    "Porém, uma pergunta surge nisso tudo: como você pode selecionar os exemplos mais relevantes para o seu contexto?\n",
    "\n",
    "É isso que iremos ver aqui."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cayena/.cache/pypoetry/virtualenvs/langchain-examples-NuBMPSch-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=gemini_api_key, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seletor por tamanho de texto (select by length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "exemplos = [\n",
    "    {\"input\": \"feliz\", \"output\": \"triste\"},\n",
    "    {\"input\": \"grande\", \"output\": \"pequeno\"},\n",
    "    {\"input\": \"energético\", \"output\": \"Letárgico\"},\n",
    "    {\"input\": \"ensolarado\", \"output\": \"nublado\"},\n",
    "    {\"input\": \"doce\", \"output\": \"salgado\"},\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\",\n",
    ")\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=exemplos,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=25\n",
    ")\n",
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Dê o antônimo da entrada do usuário e se baseie nos exemplos abaixo:\",\n",
    "    suffix=\"Input: {entrada}\\nOutput:\",\n",
    "    input_variables=[\"entrada\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dê o antônimo da entrada do usuário e se baseie nos exemplos abaixo:\n",
      "\n",
      "Input: feliz\n",
      "Output: triste\n",
      "\n",
      "Input: grande\n",
      "Output: pequeno\n",
      "\n",
      "Input: energético\n",
      "Output: Letárgico\n",
      "\n",
      "Input: ensolarado\n",
      "Output: nublado\n",
      "\n",
      "Input: doce\n",
      "Output: salgado\n",
      "\n",
      "Input: Chato\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "print(dynamic_prompt.format(entrada=\"Chato\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_llm = llm.invoke(dynamic_prompt.format(entrada=\"Chato\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interessante\n"
     ]
    }
   ],
   "source": [
    "print(resp_llm.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seletor baseado em Máxima Relevância Marginal (select by Maximum Marginal Relevance [MRR])\n",
    "\n",
    "Aqui esse seletor se baseia em distância de cosseno de Embeddings. Ele seleciona exemplos que são mais relevantes para o contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings Open Source e em Português.\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "modelPath = \"intfloat/multilingual-e5-small\"\n",
    "model_kwargs = {'device':'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=modelPath,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.prompts.example_selector import (\n",
    "    MaxMarginalRelevanceExampleSelector,\n",
    "    SemanticSimilarityExampleSelector,\n",
    ")\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_selector = MaxMarginalRelevanceExampleSelector.from_examples(\n",
    "    examples=exemplos,\n",
    "    embeddings=embeddings,\n",
    "    vectorstore_cls=FAISS,\n",
    "    k=2,\n",
    ")\n",
    "mmr_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Dê o antônimo da entrada do usuário e se baseie nos exemplos abaixo:\",\n",
    "    suffix=\"Input: {entrada}\\nOutput:\",\n",
    "    input_variables=[\"entrada\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dê o antônimo da entrada do usuário e se baseie nos exemplos abaixo:\n",
      "\n",
      "Input: grande\n",
      "Output: pequeno\n",
      "\n",
      "Input: doce\n",
      "Output: salgado\n",
      "\n",
      "Input: Alto\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "print(mmr_prompt.format(entrada=\"Alto\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testando com o SemanticSimilaritySelector\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples=exemplos,\n",
    "    embeddings=embeddings,\n",
    "    vectorstore_cls=FAISS,\n",
    "    k=1,\n",
    ")\n",
    "mmr_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Dê o antônimo da entrada do usuário e se baseie nos exemplos abaixo:\",\n",
    "    suffix=\"Input: {entrada}\\nOutput:\",\n",
    "    input_variables=[\"entrada\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dê o antônimo da entrada do usuário e se baseie nos exemplos abaixo:\n",
      "\n",
      "Input: grande\n",
      "Output: pequeno\n",
      "\n",
      "Input: Alto\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "print(mmr_prompt.format(entrada=\"Alto\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input': 'grande', 'output': 'pequeno'}]\n"
     ]
    }
   ],
   "source": [
    "# Selecionando os mais próximos via embeddings:\n",
    "selected_examples = example_selector.select_examples({\"entrada\": \"Alto\"})\n",
    "print(selected_examples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-examples-NuBMPSch-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
